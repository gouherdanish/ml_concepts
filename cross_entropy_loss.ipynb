{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[1,0.5,2],[0.5,3,1]]) # Logits\n",
    "y = torch.tensor([2,1])                 # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1 - Softmax -> Log -> CE Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Calculate Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return z.exp()/(z.exp().sum(-1)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/rc_qpxgd5z5csn7pnpmqsd8r0000gp/T/ipykernel_16169/2607686067.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p, p_pytorch = softmax(z), nn.functional.softmax(z)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2312, 0.1402, 0.6285],\n",
       "         [0.0674, 0.8214, 0.1112]]),\n",
       " tensor([[0.2312, 0.1402, 0.6285],\n",
       "         [0.0674, 0.8214, 0.1112]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.tensor([[1,0.5,2],[0.5,3,1]])\n",
    "p, p_pytorch = softmax(z), nn.functional.softmax(z)\n",
    "p, p_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses Normalized Softmax function which handles stability issues with exponentiation due to large or highly negative logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Apply log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4644, -1.9644, -0.4644],\n",
       "        [-2.6967, -0.1967, -2.1967]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp = torch.log(p)\n",
    "logp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Dot Product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Case 1 - Output is given in One-Hot Encoded form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[0,0,1],[0,1,0]],dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(logp,y):\n",
    "    return torch.tensor([-torch.dot(logp_,y_) for logp_,y_ in zip(logp,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4644, 0.1967])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll_loss(logp,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Case 2 - Output is given in class number form_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([2,1]) \n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(logp,y):\n",
    "    return -logp[range(y.shape[0]),y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4644, 0.1967])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll_loss(logp,y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Calculate Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3306), tensor(0.3306))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss, ce_loss_pytorch = loss.mean(), nn.functional.nll_loss(logp,y)\n",
    "ce_loss, ce_loss_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2 - LogSoftmax -> CE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[1,0.5,2],[0.5,3,1]]) # Logits\n",
    "y = torch.tensor([2,1])                 # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Calculate LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(z):\n",
    "    z_max, _ = torch.max(z,-1,keepdim=True)\n",
    "    return z - z_max - (z-z_max).exp().sum(-1).log().unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/rc_qpxgd5z5csn7pnpmqsd8r0000gp/T/ipykernel_16169/4075189892.py:1: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logp, logp_pytorch = log_softmax(z), nn.functional.log_softmax(z)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4644, -1.9644, -0.4644],\n",
       "         [-2.6967, -0.1967, -2.1967]]),\n",
       " tensor([[-1.4644, -1.9644, -0.4644],\n",
       "         [-2.6967, -0.1967, -2.1967]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp, logp_pytorch = log_softmax(z), nn.functional.log_softmax(z)\n",
    "logp, logp_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Calculate CE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(logp,y):\n",
    "    return -logp[range(len(y)),y].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3306), tensor(0.3306))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss, ce_loss_pytorch = nll_loss(logp,y), nn.functional.nll_loss(logp,y)\n",
    "ce_loss, ce_loss_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3 - Calculate CE Loss Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[1,0.5,2],[0.5,3,1]]) # Logits\n",
    "y = torch.tensor([2,1])                 # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3306)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_loss = nn.functional.cross_entropy(z,y)\n",
    "ce_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
